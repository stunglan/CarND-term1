# The script used to create and train the model.

# check: https://carnd-forums.udacity.com/questions/26214464/behavioral-cloning-cheatsheet
#  

from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape, Lambda,Dropout
from keras.layers.convolutional import Convolution2D
from keras.utils import np_utils
from keras.layers.advanced_activations import ELU


from PIL import Image
import cv2
import numpy as np
import json
import argparse
import time

import tensorflow as tf
tf.python.control_flow_ops = tf
tf.logging.set_verbosity(tf.logging.ERROR)
rows = 40
cols = 160
channels = 3

data_dir = './data/combined_data/'
#data_dir = './data/mattew_data/'

test_data_dir = './data/test_data/'

# TODO: import the test data

def read_image(filename):
    
    newimage = Image.open(filename)
    newimage = newimage.resize((160,80))
    image_array = np.asarray(newimage)
    image_array = image_array[30:70, 0:160]
    image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2YUV)   
    image_array = cv2.GaussianBlur(image_array, (5,5), 0)
    
    
    return image_array


def load_data(X,y,path):
    print('start loading data...')
    t = time.process_time()
    side_camera_adjustment = .3*np.random.random_sample()
    i = 0
    f = open(path + '/driving_log.csv')
    for line in f:
        line = line.split(',')
        im = read_image(path+'/'+line[0].strip())
        #print('im shape: ',im.shape)
        X.append(im)
        angle = float(line[3])
        y.append(angle)
        #left camera
        if line[1].strip():
            X.append(read_image(path+'/'+line[1].strip()))
            y.append(angle - side_camera_adjustment)
        #right camera
        if line[2].strip():
            X.append(read_image(path+'/'+line[2].strip()))
            y.append(angle + side_camera_adjustment)
    f.close()
    elapsed_time = time.process_time() - t
    print('end loading data ---~> {}'.format(elapsed_time))
    return X,y
        

def create_nvidia_model():
    """The nividia model 
    http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf
    """
    model = Sequential()
    # Use a lambda layer to normalize the input data
    model.add(Lambda(
        lambda x: x/127.5 - 1.,
        input_shape=(rows, cols, channels),
        output_shape=(rows, cols, channels))
    )

    # Several convolutional layers, each followed by ELU activation
    # ELU http://image-net.org/challenges/posters/JKU_EN_RGB_Schwarz_poster.pdf
    act = ELU
    
    # 8x8 convolution (kernel) with 4x4 stride over 16 output filters
    model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode="same"))
    model.add(act())
    # 5x5 convolution (kernel) with 2x2 stride over 32 output filters
    model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode="same"))
    model.add(act())
    # 5x5 convolution (kernel) with 2x2 stride over 64 output filters
    model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode="same"))
    # Flatten the input to the next layer
    model.add(Flatten())
    # Apply dropout to reduce overfitting
    model.add(Dropout(.2))
    model.add(act())
    # Fully connected layer
    model.add(Dense(512))
    # More dropout
    model.add(Dropout(.5))
    model.add(act())
    # Fully connected layer with one output dimension (representing the speed).
    model.add(Dense(1))
    
    model.compile('adam', 'mean_squared_error', ['accuracy'])
    return model




if __name__ == "__main__":

    # command line argument
    parser = argparse.ArgumentParser()
    parser.add_argument('-v','--verbosity',default=1,help='how much output',type=int, choices=[0, 1, 2])
    args = parser.parse_args()

    # load the data 
    X = []
    y = []
    X,y = load_data(X,y,data_dir)
    X = np.array(X).astype('float32')
    y = np.array(y).astype('float32')
    if args.verbosity >= 1:
        print('X.shape: ',X.shape)
        print('Y.shape: ',y.shape)

    
    # build the model
    mymodel = create_nvidia_model()

    # train the model
    history = mymodel.fit(X,y,nb_epoch=5,shuffle=True, batch_size=64, verbose=1,validation_split=0.1)
    #history = mymodel.fit(X,y,nb_epoch=5, shuffle=True,verbose=1,validation_split=0.1)

    # display the model
    if args.verbosity >= 2:
        print(mymodel.summary())
        print(history.history)
    
    
    # export the model and weight as required in the assignment
    if args.verbosity >= 1:
        print('Dumping model and weights...') 
    with open('model.json', 'w') as outfile:
        json.dump(mymodel.to_json(), outfile)
    mymodel.save_weights('model.h5')


    # test on other pictures
    if args.verbosity >= 1:
        print('Comparing 3 frames...') 

        X_test = []
        y_test = []
        X_test,y_test = load_data(X_test,y_test,test_data_dir)
        X_test = np.array(X_test).astype('float32')
        y_test = np.array(y_test).astype('float32')
        print('X_test.shape: ',X_test.shape)
        print('Y_test.shape: ',y_test.shape)
        
        y_prediction = mymodel.predict(X_test,verbose=1)
        
        for i in range(len(y_prediction)):
            print('predictions vs actual: {}: {:5.3f} should be {:5.3f}'.format(i, y_prediction[i][0], y_test[i]))
            
            
    # Explicitly reap session to avoid an AttributeError sometimes thrown by
    # TensorFlow on shutdown. See:
    # https://github.com/tensorflow/tensorflow/issues/3388
    from keras import backend as K
    K.clear_session()
    
        
